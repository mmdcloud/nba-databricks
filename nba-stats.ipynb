{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82f1e53-c75c-4817-a0d4-36774de563d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Inferring Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a33f135-fd8f-4f5b-b57a-c20bef072e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Define Schema (for production, avoid inferSchema)\n",
    "# ---------------------------------------------\n",
    "nba_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"team_abbreviation\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"player_height\", DoubleType(), True),\n",
    "    StructField(\"player_weight\", DoubleType(), True),\n",
    "    StructField(\"college\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"draft_year\", StringType(), True),\n",
    "    StructField(\"draft_round\", StringType(), True),\n",
    "    StructField(\"draft_number\", StringType(), True),\n",
    "    StructField(\"gp\", IntegerType(), True),\n",
    "    StructField(\"pts\", DoubleType(), True),\n",
    "    StructField(\"reb\", DoubleType(), True),\n",
    "    StructField(\"ast\", DoubleType(), True),\n",
    "    StructField(\"net_rating\", DoubleType(), True),\n",
    "    StructField(\"oreb_pct\", DoubleType(), True),\n",
    "    StructField(\"dreb_pct\", DoubleType(), True),\n",
    "    StructField(\"usg_pct\", DoubleType(), True),\n",
    "    StructField(\"ts_pct\", DoubleType(), True),\n",
    "    StructField(\"ast_pct\", DoubleType(), True),\n",
    "    StructField(\"season\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be4affd-26aa-48c0-bff1-e1b172b5d235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading CSV File using AutoLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc9df35c-5f60-4a80-9917-41009b4db05e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path = \"dbfs:/Volumes/workspace/default/nba/\"   # replace with your mounted volume path\n",
    "\n",
    "nba_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(nba_schema)\n",
    "    .option(\"cloudFiles.schemaLocation\", \"dbfs:/Volumes/workspace/default/nba/\")\n",
    "    .load(input_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e353c363-0230-4a64-8b88-3958256572dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db4bd14-3322-467b-a0aa-8c8f6b4be840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nba_clean = (nba_df\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "    .withColumn(\"player_height_m\", round(col(\"player_height\")/100, 2))\n",
    "    .withColumn(\"player_weight_kg\", round(col(\"player_weight\")/2.205, 1))\n",
    "    .drop(\"id\")   # drop junk column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9883f4e9-4ba5-494b-a713-02394a27853b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write To Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a3c8d5-a1d9-4127-93e6-1102ab8ff88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(nba_clean.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/checkpoints/nba_checkpoint/\")\n",
    "    .option(\"path\", \"dbfs:/mnt/processed/nba_delta/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf28ebe-7b08-49f3-b79e-6d146c764aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analytics Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112412f1-e8a5-4f6d-97e6-4752c321dfb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nba_clean.createOrReplaceTempView(\"nba\")\n",
    "\n",
    "# Example queries:\n",
    "\n",
    "# Top scorers by season\n",
    "top_scorers = spark.sql(\"\"\"\n",
    "SELECT season, player_name, team_abbreviation, ROUND(pts,2) as points\n",
    "FROM nba\n",
    "ORDER BY season, points DESC\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "# Team average stats\n",
    "team_summary = spark.sql(\"\"\"\n",
    "SELECT season, team_abbreviation,\n",
    "       ROUND(AVG(pts),2) as avg_pts,\n",
    "       ROUND(AVG(reb),2) as avg_reb,\n",
    "       ROUND(AVG(ast),2) as avg_ast\n",
    "FROM nba\n",
    "GROUP BY season, team_abbreviation\n",
    "ORDER BY season, avg_pts DESC\n",
    "\"\"\")\n",
    "\n",
    "# Country-wise player distribution\n",
    "country_dist = spark.sql(\"\"\"\n",
    "SELECT country, COUNT(DISTINCT player_name) as players\n",
    "FROM nba\n",
    "GROUP BY country\n",
    "ORDER BY players DESC\n",
    "\"\"\")\n",
    "\n",
    "display(top_scorers)\n",
    "display(team_summary)\n",
    "display(country_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1da8076-6825-4951-acfc-feb73a200670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Efficiency crackdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d465e9f8-612e-4ac5-b9c4-edc7c9625ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "efficiency = nba_clean.select(\"player_name\", \"season\", \"usg_pct\", \"ts_pct\", \"ast_pct\")\n",
    "display(efficiency)\n",
    "\n",
    "# College influence\n",
    "college_perf = spark.sql(\"\"\"\n",
    "SELECT college, ROUND(AVG(pts),2) as avg_points, COUNT(DISTINCT player_name) as players\n",
    "FROM nba\n",
    "WHERE college IS NOT NULL AND college <> 'None'\n",
    "GROUP BY college\n",
    "ORDER BY avg_points DESC\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "display(college_perf)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nba-stats",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
